# Deep-learning-from-scratch
This project implements a deep learning library from scratch, which provides functionalities similar to those of PyTorch. The library includes support for linear layers, ReLU and Sigmoid activation functions, as well as various loss functions such as crossentropy, hinge loss, and mean squared loss. Additionally, it offers options for weight initialization, including zero weight initialization and normal weight initialization. This project implements the Simpletorch library, which provides functionalities similar to those of PyTorch. The library includes support for linear layers, ReLU and Sigmoid activation functions, as well as various loss functions such as crossentropy, hinge loss, and mean squared loss. Additionally, it offers options for weight initialization, including zero weight initialization and normal weight initialization.This project implements the Simpletorch library, which provides functionalities similar to those of PyTorch. The library includes support for linear layers, ReLU and Sigmoid activation functions, as well as various loss functions such as crossentropy, hinge loss, and mean squared loss. Additionally, it offers options for weight initialization, including zero weight initialization and normal weight initialization.
The library has implementation of reverse mode automatic differentiation. In the forward pass, it will create a computation graph. And during the backward pass, this graph is traveresed recursively to calculate gradient using chain multiplication
